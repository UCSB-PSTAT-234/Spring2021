{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix Factorizations and Data Analysis<a class=\"tocSkip\">\n",
    "\n",
    "## Sang-Yun Oh <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dimensionality and Rank\n",
    "\n",
    "- Rank of a matrix $\\approx$ effective dimensionality of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Many features does not mean data is rich:\n",
    "    there may be redundant information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Matrices could be low-rank: e.g. rank-one approximation of $R = vu^T$\n",
    "    $$\n",
    "    \\begin{array}{|l|l|l|l|}\\hline \n",
    "            & u_1 & u_2 & u_3 \\\\ \\hline \n",
    "        m_1 & 1 & 0 & 2 \\\\ \\hline \n",
    "        m_2 & 2 & 0 & 4 \\\\ \\hline \n",
    "        m_3 & 3 & 0 & 6 \\\\ \\hline \n",
    "    \\end{array}\n",
    "    = R = uv^T =\n",
    "    \\begin{array}{|l|l|}\\hline \n",
    "        m_1 & 1  \\\\ \\hline \n",
    "        m_2 & 2  \\\\ \\hline \n",
    "        m_3 & 3  \\\\ \\hline \n",
    "    \\end{array}\n",
    "    \\begin{array}{|l|l|l|}\\hline \n",
    "            u_1 & u_2 & u_3 \\\\ \\hline \n",
    "            1 & 0 & 2 \\\\ \\hline \n",
    "    \\end{array}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Large matrix does not mean rank is high: there may be linearly dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Linear dependency on other features:  \n",
    "    Some columns maybe linear combination of others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear dependence and Redundant information\n",
    "\n",
    "- Linear combination of vectors:  \n",
    "$$\n",
    "\\frac{1}{10} \\cdot \\left[ \\begin{array}{l}{2} \\\\ {3} \\\\ {4}\\end{array}\\right]+2 \\cdot \\left[ \\begin{array}{l}{5} \\\\ {7} \\\\ {9}\\end{array}\\right]=\\left[ \\begin{array}{l}{10.2} \\\\ {14.3} \\\\ {18.4}\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A matrix (mxn) times a column (nx1) gives  \n",
    "    one linear combination of the columns of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A matrix (mxn) times a matrix (nxk) has k columns that are  \n",
    "    each a matrix (mxn) times a column (nx1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Two height data columns are linear combination of each other\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|}\\hline \\text { Age (days) } & {\\text { Height (in) }} \\\\ \\hline 182 & {28} \\\\ \\hline 399 & {30} \\\\ \\hline 725 & {33} \\\\ \\hline\\end{array}\n",
    "\\times\n",
    "\\begin{array}{|l|l|l|}\\hline 1 & {0} & {0} \\\\ \\hline 0 & {1} & {1 / 12} \\\\ \\hline\\end{array}\n",
    "=\n",
    "\\begin{array}{|c|c|c|}\\hline \\text { Age (days) } & {\\text { Height (in) }} & {\\text { Height }(\\mathrm{ft})} \\\\ \\hline 182 & {28} & {2.33} \\\\ \\hline 399 & {30} & {2.5} \\\\ \\hline 725 & {33} & {2.75} \\\\ \\hline\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\small\n",
    "\\begin{array}{|l|l|}\\hline \\text { width } & {\\text { length }} & {\\text { area }} \\\\ \\hline 20 & {20} & {400} \\\\ \\hline 16 & {12} & {192} \\\\ \\hline 24 & {12} & {288} \\\\ \\hline 25 & {24} & {600} \\\\ \\hline\\end{array}\n",
    "\\times\n",
    "\\begin{array}{|c|c|c|c|}\\hline 1 & {0} & {0} & {2} \\\\ \\hline 0 & {1} & {0} & {2} \\\\ \\hline 0 & {0} & {1} & {0} \\\\ \\hline\\end{array}\n",
    "=\n",
    "\\begin{array}{|l|l|l|}\\hline \\text { width } & {\\text { length }} & {\\text { area }} & {\\text { perimeter }} \\\\ \\hline 20 & {20} & {400} & {80} \\\\ \\hline 16 & {12} & {192} & {60} \\\\ \\hline 24 & {12} & {288} & {72} \\\\ \\hline\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What if columns are not *perfect* linear combinations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Columns may be *approximately* a linear combination of others (numerical rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recommender Systems\n",
    "\n",
    "### Latent Factor approach for Recommender System\n",
    "\n",
    "![](images/recommender-latent-factor.png)\n",
    "\n",
    "[image source](https://ieeexplore.ieee.org/document/5197422)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Latent Factor Representation\n",
    "\n",
    "![](images/movie-latent-factors.png)\n",
    "\n",
    "[image source](https://ucsb-primo.hosted.exlibrisgroup.com/permalink/f/1egv95m/01UCSB_ALMA51268902820003776)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Ratings is a combination of user and movie characteristics:  \n",
    "    $$\\begin{aligned} r_{i j} & \\approx \\sum_{s=1}^{k} u_{i s} \\cdot v_{j s} \\\\ &=\\sum_{s=1}^{k}(\\text { Affinity of user $i$ to characteristic }s) \\times(\\text {Affinity of movie } j \\text { to characteristic} s) \\end{aligned}$$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "- For User $i$'s rating of Movie $j$,  \n",
    "    $$\n",
    "    \\begin{aligned} r_{i j} \\approx &(\\text {Affinity of user $i$ to history}) \\times(\\text {Affinity of item } j \\text { to history}) \\\\ &+(\\text {Affinity of user $i$ to romance)} \\times(\\text {Affinity of item } j\\text { to romance) } \\end{aligned}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Latent Factor Representation?\n",
    "\n",
    "- $R$ is _approximately_ low rank: i.e. $U$ and $V$ are rank-2 matrices\n",
    "    $$ R\\approx UV^T, $$  \n",
    "\n",
    "- Find $U$ and $V$ to minimize matrix norm of residual matrix: e.g.,\n",
    "    $$ \\min_{U,V} \\|R - UV^T\\|_F = \\min_{U,V} \\sum_{i,j} (r_{ij} - u_i^T v_j)^2$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  \n",
    "Ratings | Residual\n",
    "- | - \n",
    "![](images/latent-factor-ratings.png) | ![](images/latent-factor-residual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "|                               |                                |\n",
    "|------------------------------ | ------------------------------ |\n",
    "|![matmul1](images/matmul1.png) | ![matmul1](images/matmul2.png) |\n",
    "|$$ C_{ij}=A_{i-}^TB_{-j}=\\sum_{k=1}^K A_{ik}B_{kj} $$ |$$ C=\\sum_{k=1}^K A_{-k}B_{k-} $$ |\n",
    "|![matmul3](images/matmul3.png) | ![matmul4](images/matmul4.png) |\n",
    "|$$ C_{i-}=A_{i-}B $$           |$$C_{-j} = AB_{-j}$$            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix Decompositions: Principal Components Analysis\n",
    "\n",
    "$X$: Data matrix of size $\\mathbb{R}^{n\\times p}$\n",
    "\n",
    "- Principal Components Analysis (PCA): $ X = Q Y $\n",
    "    \n",
    "    + $Q$: Orthonormal rotation matrix (loadings)\n",
    "    + $Y$: Rotated data matrix (score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Rotation matrix $Q$ is computed to transform data $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First columns of $Y$ contain a larger proportion of _information_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- PCA can be described in terms of SVD factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-negative Matrix Factorization\n",
    "\n",
    "- Assume data $X$ is $p\\times n$ matrix of non-negative values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- e.g., images, probabilities, counts, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- NMF computes the following factorization:  \n",
    "$$ \\min_{W,H} \\| X - WH \\|_F\\\\\n",
    "\\text{ subject to } W\\geq 0,\\ H\\geq 0, $$  \n",
    "    where $W$ is ${p\\times r}$ matrix and $H$ is ${r\\times n}$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NMF for Image Analysis\n",
    "\n",
    "![nmf-faces](images/nmf-faces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NMF for Hyperspectral image analysis\n",
    "\n",
    "![nmf-hyper](images/nmf-hyper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### NMF for Topic Discovery\n",
    "\n",
    "![nmf-topics](images/nmf-topics.png)\n",
    "\n",
    "\n",
    "- [More NMF examples](https://www.cs.rochester.edu/u/jliu/CSC-576/NMF-tutorial.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Matrix Factorizations\n",
    "\n",
    "- [Singular Value Decomposition](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)\n",
    "\n",
    "- [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "- [Independent Component Analysis](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition)  \n",
    "    [Blind Source Separation](https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html)\n",
    "\n",
    "- [Non-negative Matrix Factorization](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF)  \n",
    "    [Topic Discovery](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html)\n",
    "    [Image Analysis](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html)\n",
    "\n",
    "- [Matrix Decompositions](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- [A Tutorial on Principal Component Analysis, Jonathon Shlens](https://arxiv.org/abs/1404.1100)\n",
    "\n",
    "- [A Tutorial on Independent Component Analysis, Jonathon Shlens](https://arxiv.org/abs/1404.2986)\n",
    "\n",
    "- UC Berkeley's Data Science 100 lecture notes, John Denero\n",
    "\n",
    "- [The Why and How of Nonnegative Matrix Factorization - Nicolas Gillis](https://arxiv.org/abs/1401.5226)\n",
    "\n",
    "- [Matrix-based introduction to multivariate data analysis - Adachi](https://doi.org/10.1007/978-981-15-4103-2)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
